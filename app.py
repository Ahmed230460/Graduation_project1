# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qv6GAyxleaYCifsb0sB4IC5qxeoTbY6n
"""

import streamlit as st
import cv2
import numpy as np
import av
import torch
from ultralytics import YOLO
from transformers import VivitForVideoClassification, VivitImageProcessor
import google.generativeai as genai
import os
import io
from PIL import Image
from transformers import MarianMTModel, MarianTokenizer

# تحميل المودلز من مصادر خارجية
@st.cache_resource
def load_models():
    # YOLOv12
    yolo_model = YOLO("yolov8n.pt")  # يتحمل من ultralytics مباشرة
    
    # ViViT
    processor = VivitImageProcessor.from_pretrained("prathameshdalal/vivit-b-16x2-kinetics400-UCF-Crime")
    vivit_model = VivitForVideoClassification.from_pretrained("prathameshdalal/vivit-b-16x2-kinetics400-UCF-Crime")

    # Gemini (مع API Key)
    genai.configure(api_key="AIzaSyCZFf2r-fmE9uRQjKebHfF_MZhDKwiZP7A")
    gemini_model = genai.GenerativeModel("gemini-1.5-flash")

    return yolo_model, processor, vivit_model, gemini_model

# تابع Preprocessing
def preprocess_video(input_path, output_path, target_size=(224, 224)):
    cap = cv2.VideoCapture(input_path)
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, target_size)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)
        normalized_frame = resized_frame / 255.0
        output_frame = (normalized_frame * 255).astype(np.uint8)
        out.write(output_frame)

    cap.release()
    out.release()
    return output_path

# تابع استخراج Keyframes
def extract_keyframes(video_path, output_path="significant_keyframes_output.mp4"):
    cap = cv2.VideoCapture(video_path)
    ret, prev_frame = cap.read()
    if not ret:
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    frame_count = 0
    event_active = False
    motion_history = []
    saved_frames = 0
    peak_frame = None

    yolo_model = load_models()[0]  # YOLO Model

    while cap.isOpened():
        ret, current_frame = cap.read()
        if not ret:
            if event_active and peak_frame is not None:
                out.write(peak_frame)
                saved_frames += 1
            break

        frame_count += 1
        current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        diff_gray = cv2.absdiff(prev_gray, current_gray)
        _, thresh_gray = cv2.threshold(diff_gray, 30, 255, cv2.THRESH_BINARY)
        non_zero_count = cv2.countNonZero(thresh_gray)
        motion_history.append(non_zero_count)
        if len(motion_history) > 50:
            motion_history.pop(0)
        adaptive_threshold = max(100, np.mean(motion_history) * 2)

        motion_detected = non_zero_count > adaptive_threshold

        if motion_detected:
            if not event_active:
                event_active = True
            peak_frame = current_frame
        elif event_active and frame_count > 30:
            event_active = False
            if peak_frame is not None:
                out.write(peak_frame)
                saved_frames += 1

        prev_gray = current_gray
        prev_frame = current_frame.copy()

    cap.release()
    out.release()
    return output_path

# تابع Anomaly Detection
def anomaly_detection(video_path, processor, vivit_model):
    container = av.open(video_path)
    frames = [frame.to_image() for frame in container.decode(video=0)]

    required_frames = 32
    if len(frames) < required_frames:
        repeat_factor = (required_frames + len(frames) - 1) // len(frames)
        frames = (frames * repeat_factor)[:required_frames]
    elif len(frames) > required_frames:
        frames = frames[:required_frames]

    inputs = processor(frames, return_tensors="pt")
    with torch.no_grad():
        outputs = vivit_model(**inputs)
        logits = outputs.logits
        predicted_class = logits.argmax(-1).item()

    return vivit_model.config.id2label[predicted_class]

# تابع Story Generation
def generate_story(video_path, prediction, gemini_model):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = max(1, total_frames // 15)
    frames = []
    frame_idx = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % step == 0:
            frame_path = f"frame_{frame_idx}.jpg"
            cv2.imwrite(frame_path, frame)
            frames.append(frame_path)
        frame_idx += 1
    cap.release()

    descriptions = {}
    for frame_path in frames:
        prompt = f"This frame is from a video classified as '{prediction}'. Describe the event in one sentence."
        with open(frame_path, "rb") as img_file:
            image_data = Image.open(io.BytesIO(img_file.read()))
        response = gemini_model.generate_content([prompt, image_data])
        descriptions[frame_path] = response.text

    summary_prompt = "Here are multiple descriptions of frames from a video:\n" + "\n".join(descriptions.values()) + "\nProvide a concise summary of the overall event."
    summary_response = gemini_model.generate_content(summary_prompt)
    return summary_response.text

# الواجهة الرئيسية
def main():
    st.title("Video Story Generator")
    uploaded_file = st.file_uploader("ارفع فيديو", type=["mp4", "avi"])

    if uploaded_file is not None:
        with open("input_video.mp4", "wb") as f:
            f.write(uploaded_file.read())

        st.write("جاري المعالجة...")
        yolo_model, processor, vivit_model, gemini_model = load_models()

        # Preprocessing
        preprocessed_video = preprocess_video("input_video.mp4", "preprocessed_video.mp4")

        # Keyframe Extraction
        keyframes_video = extract_keyframes(preprocessed_video)

        # Anomaly Detection
        prediction = anomaly_detection(keyframes_video, processor, vivit_model)
        st.write(f"التصنيف: {prediction}")

        # Story Generation
        story = generate_story(keyframes_video, prediction, gemini_model)
        st.write("القصة المولدة:")
        st.write(story)

if __name__ == "__main__":
    main()
